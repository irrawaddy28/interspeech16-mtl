\relax 
\citation{Jyothi-MismatchedCrowdsourcingTrans}
\citation{Jyothi-MismatchedCrowdsourcingTrans}
\citation{Stolcke-DNNPostFeatures,Thomas-DNNPostFeatures}
\citation{Grezl-BNFFirstPaper,Thomas-BNFXlingual}
\citation{Vu-MLPInitSchemes}
\citation{Knill-SelfTrainingAndUnsupAdapt}
\citation{Hasegawa-WS15presentation}
\citation{Jyothi-MismatchedCrowdsourcingTrans}
\@writefile{toc}{\contentsline {section}{\numberline {1} Introduction}{1}}
\newlabel{sec:Introduction}{{1}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {2} Algorithm}{1}}
\newlabel{sec:Algorithm}{{2}{1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1} Mismatched crowdsourcing}{1}}
\citation{Vesely-SemisupTrainingDNN}
\citation{Knill-SelfTrainingAndUnsupAdapt}
\citation{Knill-SelfTrainingAndUnsupAdapt}
\citation{Huang-MultilingualSHL}
\citation{Ghoshal-MultilingualPretraining}
\citation{Yu-FeatureLearning}
\citation{Seltzer-MTLPhonemeRecog}
\citation{Scanzio-MultisoftmaxFirstPaper}
\citation{Vesely-MultilingualBNF}
\citation{Huang-MultilingualSHL}
\citation{Seltzer-MTLPhonemeRecog}
\citation{Scanzio-MultisoftmaxFirstPaper}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A deterministic transcription (DT) for the word \emph  {cat}.\relax }}{2}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:dt}{{1}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces A probabilistic transcription (PT) for the word \emph  {cat}.\relax }}{2}}
\newlabel{fig:pt}{{2}{2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2} DNN Training using Probabilistic Transcripts}{2}}
\newlabel{sec:DNN Training using Probabilistic Transcripts}{{2.2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces DNN adaptation to probabilistic transcripts (PT).\relax }}{2}}
\newlabel{fig:DNN}{{3}{2}}
\citation{Jyothi-MismatchedCrowdsourcingTrans}
\citation{Gopinath-MLLT}
\citation{Gales-CMLLR}
\citation{Bengio-Pretraining}
\citation{Povey-Kaldi}
\@writefile{toc}{\contentsline {section}{\numberline {3} Experiments and Results}{3}}
\newlabel{sec:Experiments and Results}{{3}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1} Data}{3}}
\newlabel{sec:Data}{{3.1}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2} Monolingual HMM and DNN}{3}}
\newlabel{sec:Monolingual HMM and DNN}{{3.2}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces SBS Multilingual Corpus.\relax }}{3}}
\newlabel{Tab:Turkish and English Phoneme Set}{{1}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces PERs of monolingual HMM and DNN models. Dev set in parentheses.\relax }}{3}}
\newlabel{Tab:PER_Matched_Monolingual}{{2}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces PERs of multilingual HMM and DNN models. Dev set in parentheses.\relax }}{3}}
\newlabel{Tab:PER_Mismatched_Multilingual}{{3}{3}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces PERs of self-trained DNN models trained using STs. Dev set in parentheses.\relax }}{3}}
\newlabel{Tab:PER_ASRPT_DNN_monosoftmax}{{4}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3} Multilingual HMM and DNN}{3}}
\newlabel{sec:Multilingual HMM and DNN}{{3.3}{3}}
\citation{Vesely-SemisupTrainingDNN}
\citation{Knill-SelfTrainingAndUnsupAdapt}
\citation{Liu-PTAdaptedGMM}
\citation{Ghoshal-MultilingualPretraining}
\@writefile{lot}{\contentsline {table}{\numberline {5}{\ignorespaces PERs of HMM, DNN-1, DNN-2, DNN-3 models trained using PTs. First element in parentheses is the PER of the dev set. Second element is the absolute improvement in PER of the test set over MAP HMM.\relax }}{4}}
\newlabel{Tab:PER_PT}{{5}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4} Self-training DNN}{4}}
\newlabel{sec:Self-training}{{3.4}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5} Training one softmax DNNs using PT: DNN-1}{4}}
\newlabel{sec:Effect of training single softmax DNNs using crowdsource PT data}{{3.5}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6} Training two softmax DNNs using PT and DT: DNN-2}{4}}
\newlabel{sec:Effect of training two softmax DNNs using crowdsource PT and multilingual DT data}{{3.6}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7} Training three softmax DNNs using PT, DT, and ST: DNN-3}{4}}
\newlabel{sec:Effect of training three softmax DNNs using crowdsourced PT, multilingual DT, and unsupervised data}{{3.7}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {4} Conclusions}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {5} Acknowledgements}{4}}
\bibstyle{IEEEtran}
\bibdata{IEEEabrv,IEEEexample}
\bibcite{Jyothi-MismatchedCrowdsourcingTrans}{1}
\bibcite{Stolcke-DNNPostFeatures}{2}
\bibcite{Thomas-DNNPostFeatures}{3}
\bibcite{Grezl-BNFFirstPaper}{4}
\bibcite{Thomas-BNFXlingual}{5}
\bibcite{Vu-MLPInitSchemes}{6}
\bibcite{Knill-SelfTrainingAndUnsupAdapt}{7}
\bibcite{Hasegawa-WS15presentation}{8}
\bibcite{Vesely-SemisupTrainingDNN}{9}
\bibcite{Huang-MultilingualSHL}{10}
\bibcite{Ghoshal-MultilingualPretraining}{11}
\bibcite{Yu-FeatureLearning}{12}
\bibcite{Seltzer-MTLPhonemeRecog}{13}
\bibcite{Scanzio-MultisoftmaxFirstPaper}{14}
\bibcite{Vesely-MultilingualBNF}{15}
\bibcite{Gopinath-MLLT}{16}
\bibcite{Gales-CMLLR}{17}
\bibcite{Bengio-Pretraining}{18}
\bibcite{Povey-Kaldi}{19}
\bibcite{Liu-PTAdaptedGMM}{20}
\@writefile{toc}{\contentsline {section}{\numberline {6} References}{5}}
